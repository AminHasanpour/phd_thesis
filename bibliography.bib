@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group UK London}
}
@article{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}
@misc{MCUFragmentation,
   author = {Embedded Staff},
   title = {Consolidating the MCU market around the ARM architecture},
   url = {https://www.embedded.com/consolidating-the-mcu-market-around-the-arm-architecture},
   note = {accessed: 27 December, 2025},
}
@inproceedings{hasanpour2025survey,
  title={A Survey of Quantization Techniques in Embedded AI Toolchains},
  author={Hasanpour, Mohammad Amin and Fafoutis, Xenofon and Roveri, Manuel},
  booktitle={2025 IEEE Annual Congress on Artificial Intelligence of Things},
  year={2025},
  organization={IEEE}
}

% ==============================================================================
% Paper Cavitation
% ==============================================================================

@article{Obaidi2020_1,
   abstract = {Cavitation is an important problem that occurs in any pump and contributes highly towards the deterioration in the performance of the pump. In industrial applications, it is vital to detect and decrease the effect of cavitation in pumps. In this study therefore, focus is on detecting and diagnosing the cavitation phenomenon within a centrifugal pump using vibration technique. The results obtained for vibration signal in time and frequency domains have been analysed in order to achieve a better understanding regarding detection of cavitation within a pump. The effect of different operating conditions, including various flow rates related to the cavitation have been investigated in this work using different statistical features in time domain analysis (TDA). Moreover, Fast Fourier Transform (FFT) technique for frequency domain analysis (FDA) has also been applied.},
   author = {Ahmed Ramadhan Al-Obaidi},
   doi = {10.1007/s40799-020-00362-z},
   issn = {17471567},
   issue = {3},
   journal = {Experimental Techniques},
   keywords = {Cavitation,Centrifugal pump,Fast Fourier Transform (FFT),Vibration technique},
   pages = {329-347},
   publisher = {Springer},
   title = {Detection of Cavitation Phenomenon within a Centrifugal Pump Based on Vibration Analysis Technique in both Time and Frequency Domains},
   volume = {44},
   year = {2020},
}
@article{Dutta2018,
   abstract = {Cavitation is one of the major disadvantages in pumping system, which enhance to form bubbles in the pipeline and it reduces the efficiency of the pump. So it should be identified and take the preventive measure. Machine Learning is a fast and computational method which can easily detect any faults in the pumping system. Still now lots of work has been done on a detection of fault in the pumping system, but mainly those work has done based on vibration details and variation of speed. The paper presents how by the help of machine learning algorithm by varying the speed and pressure cavitation can be identified. It is the comparative study between how the vibration and speed together affects the cavitation result and variation of speed and pressure affects the cavitation. Support Vector Machine is one of the classification methods in machine learning algorithm where it can be easily classified the cavitation problem. So this paper analyses how the method of SVM can more efficiently detect the cavitation problem with the centrifugal water pump.},
   author = {Nabanita Dutta and Subramaniam Umashankar and V.K. Arun Shankar and Sanjeevikumar Padmanaban and Zbigniew Leonowicz and Patrick Wheeler},
   isbn = {9781538651865},
   journal = {International Conference on Environment and Electrical Engineering (EEEIC)},
   title = {Centrifugal Pump Cavitation Detection Using Machine Learning Algorithm Technique},
   year = {2018},
}
@article{Abdulaziz2017,
   abstract = {Cavitation in pumps causes destructive consequences and occurs at certain operating circumstances. Therefore, it must be detected and prevented. The main thrust of the present work is detection and investigation of cavitation in a centrifugal pump experimentally, by monitoring the flow characteristics as well as the vibration spectrum. It is an attempt to declare the discrete signature of cavitation at various strengths on the vibration spectrum. The experimental results on a centrifugal pump at a rotational speed of 2850 rpm and absolute values of suction pressures from 10 to 78 kPa, showed that the inception of cavitation is declared by sudden deterioration in the hydraulic characteristics, decrease in the efficiency and increase in the overall vibration level. As the cavitation strength increases, the vibration level decreases and then increases sharply. Cavitation can be detected by online monitoring of the vibration amplitude of the discrete frequencies corresponding to the pump rotational speed and the first blade passing. As the pump operation moves from no-cavitation to cavitation case, the vibration amplitude of the rotational speed frequency increases while it decreases for the first passing frequency. Also, the cavitation in pumps signs the vibration spectrum by appearance of high energy at high frequencies.},
   author = {A. M. Abdulaziz and Ashraf Kotb},
   doi = {10.1080/14484846.2015.1093261},
   issn = {14484846},
   issue = {2},
   journal = {Australian Journal of Mechanical Engineering},
   keywords = {Centrifugal pump,blade passing frequency,cavitation,vibration},
   month = {5},
   pages = {103-110},
   publisher = {Taylor and Francis Ltd.},
   title = {Detection of pump cavitation by vibration signature},
   volume = {15},
   year = {2017},
}
@article{Obaidi2020_2,
   abstract = {Cavitation is an essential problem that occurs in all kinds of pumps. This cavitation contributes highly towards the deterioration in the performance of the pump. In industrial applications, it is very vital to detect and decrease the effect of the cavitation in pumps. Using different techniques to analysis and diagnose cavitation leads to increase in the reliability of cavitation detection. The use of various techniques such as vibration and acoustic analyses can provide a more robust detection of cavitation within the pump. In this work therefore, focus is put on detecting and diagnosing the cavitation phenomenon within a centrifugal pump using vibration and acoustic techniques. The results obtained from vibration and acoustic signals in time and frequency domains were analysed in order to achieve better understanding regarding detection of cavitation within a pump. The effect of different operating conditions related to the cavitation was investigated in this work using different statistical features in time domain analysis (TDA). Moreover, Fast Fourier Transform (FFT) technique for frequency domain analysis (FDA) was also applied. Furthermore, the comparison and evaluation system among different techniques to find an adequate technique incorporating for accuracy and to increase the reliability of detection and diagnosing different levels of cavitation within a centrifugal pump were also investigated.},
   author = {Ahmed Ramadhan AL-OBAIDI},
   doi = {10.24425/aoa.2020.134070},
   issn = {2300262X},
   issue = {3},
   journal = {Archives of Acoustics},
   keywords = {Acoustic,Cavitation,Centrifugal pump,Fast Fourier Transform,Normalise features,Vibration},
   pages = {541-556},
   publisher = {Polska Akademia Nauk},
   title = {Experimental comparative investigations to evaluate cavitation conditions within a centrifugal pump based on vibration and acoustic analyses techniques},
   volume = {45},
   year = {2020},
}
@misc{Matloobi2021,
   abstract = {Reducing the cost of unscheduled shutdowns and enhancing the reliability of production systems is an important goal for various industries; this could be achieved by condition monitoring and artificial intelligence. Cavitation is a common undesired phenomenon in centrifugal pumps, which causes damage and its detection in the preliminary stage is very important. In this paper, cavitation is identified by use of vibration and current signal and artificial immune network that is modeled on the base of the human immune system. For this purpose, first data collection were done by a laboratory setup in health and five stages damage condition; then various features in time, frequency, and time–frequency were extracted from vibration and current signals in addition to pressure and flow rate; next feature selection and dimensions reduction were done by artificial immune method to use for classification; finally, they were used by artificial immune network and some other methods to identify the system condition and classification. The results of this study showed that this method is more accurate in the detection of cavitation in the initial stage compared to methods such as non-linear supportive vector machine, multi-layer artificial neural network, K-means and fuzzy C-means with the same data. Also, selected features with artificial immune system were better than principal component analysis results.},
   author = {Seyed M. Matloobi and Mohammad Riahi},
   doi = {10.1177/09544089211028402},
   issn = {20413009},
   issue = {6},
   journal = {Proceedings of the Institution of Mechanical Engineers, Part E: Journal of Process Mechanical Engineering},
   keywords = {Condition monitoring,artificial immune net,cavitation,clonal G},
   month = {12},
   pages = {2271-2280},
   publisher = {SAGE Publications Ltd},
   title = {Identification of cavitation in centrifugal pump by artificial immune network},
   volume = {235},
   year = {2021},
}
@article{Hajnayeb2021,
   abstract = {Detection of cavitation in centrifugal pumps is critical in their condition monitoring. In order to detect cavitation more accurately and confidently, more advanced signal processing techniques are needed. For the classification of a pump conditions based on the outputs of these techniques, advanced machine learning techniques are needed. In this research, an automatic system for cavitation detection is proposed based on machine learning. Bispectral analysis is used for analyzing the vibration signals. The resulting bispectrum images are given to convolutional neural networks (CNNs) as inputs. The CNNs are a pretrained AlexNet and a pretrained GoogleNet, which are used in this application through transfer learning. On the contrary, a laboratory test setup is used for generating controlled cavitation in a centrifugal pump. The suggested algorithm is implemented on the vibration dataset acquired from the laboratory pump test setup. The results show that the cavitation state of the pump can be detected accurately using this system without any need to image processing or feature extraction.},
   author = {Ali Hajnayeb},
   doi = {10.1155/2021/6988949},
   issn = {10709622},
   journal = {Shock and Vibration},
   publisher = {Hindawi Limited},
   title = {Cavitation Analysis in Centrifugal Pumps Based on Vibration Bispectrum and Transfer Learning},
   volume = {2021},
   year = {2021},
}
@article{Cao2021,
   abstract = {Vibration is a common side effect of cavitation in centrifugal pumps, which are also affected by other physical processes. Therefore, the study of cavitation based on raw vibration signals may introduce potential discrepancies under practical operating conditions. With the objective of accurately identifying the cavitation stages in a centrifugal pump, this study employs the wavelet packet transform (WPT) to process the original signal. Subsequently, the different vibration acceleration signals obtained by processing the raw signals using the proposed method under different working conditions are compared. The unequal interval weight grey (UIWG) model is proposed based on the measurements performed to reconstruct the functional relationship between cavitation and vibration, including in cases where the physical parameters of the research object are unavailable. The results reveal that the post-signal energy exhibits monotonic characteristics in certain frequency bands, despite the presence of interference under practical operating conditions. In addition, the UIWG model is verified to be robust as it is capable of self-correcting its monotonicity based on limited discrete data. In conclusion, the UIWG model equipped with WPT is an effective means to predict cavitation-induced vibrations.},
   author = {Ruijia Cao and Jianping Yuan and Fanjie Deng and Longyan Wang},
   doi = {10.1088/1361-6501/ac1181},
   issn = {13616501},
   issue = {11},
   journal = {Measurement Science and Technology},
   keywords = {cavitation,centrifugal pumps,unequal interval weight grey model,unstable state,vibration,wavelet packet transform},
   month = {11},
   publisher = {IOP Publishing Ltd},
   title = {Numerical method to predict vibration characteristics induced by cavitation in centrifugal pumps},
   volume = {32},
   year = {2021},
}
@article{Shervani2018,
   abstract = {The cavitation phenomenon, which is rampant in axial flow pumps, should be avoided due to its undesirable effects on the pump’s performance. Therefore, in this study the cavitation performance of an axial flow pump is monitored based on vibration signals. For this purpose, experimental vibration data is collected for five different levels of cavitation. Time-domain features are extracted based on statistical behavior of the measured signals. Considering the nonlinear and high-frequency nature of the cavitation noise in the signal, the second set of features including both time- and frequency-domain parameters are obtained based on statistical behavior of the first intrinsic mode function, via empirical mode decomposition combined with Hilbert Huang transform. Compensation distance evaluation technique is applied to pick the appropriate features. Multi-class support vector machine is trained for classification of the various levels of cavitation intensity. The results of testing the support vector machine algorithm show that the developed methodology can monitor the pump’s cavitation intensity in onsite operation with high accuracy.},
   author = {Mohammad Taghi Shervani-Tabar and Mir Mohammad Ettefagh and Saeed Lotfan and Hamed Safarzadeh},
   doi = {10.1177/0954406217729416},
   issn = {20412983},
   issue = {17},
   journal = {Proceedings of the Institution of Mechanical Engineers, Part C: Journal of Mechanical Engineering Science},
   keywords = {Vibration signal,axial flow pump,cavitation monitoring,compensation distance evaluation,empirical mode decomposition,multi-class support vector machine},
   month = {9},
   pages = {3013-3026},
   publisher = {SAGE Publications Ltd},
   title = {Cavitation intensity monitoring in an axial flow pump based on vibration signals using multi-class support vector machine},
   volume = {232},
   year = {2018},
}
@article{Mousmoulis2019,
   abstract = {The continuously increasing industrial productivity has resulted in a great breakthrough in the field of maintenance on centrifugal pumps in order to ensure their optimum operation under different operating conditions. One of the important mechanisms that affect the steady and dynamic operation of a pump is cavitation, which appears in the low static pressure zone formed at the impeller entrance region. This paper investigates the inception and development of cavitation in three different impellers of a laboratory centrifugal pump with a Plexiglas casing, using flow visualization, vibration and acoustic emission measurements. The aim of this study is the development of an experimental tool that detects cavitation in different impellers and the further understanding of the effects of blade geometry in cavitation development. The results show that the geometrical characteristics of the impeller affect cavitation development and behavior, while an acoustic emission sensor and an accelerometer can be applied for successfully detecting the onset of this mechanism.},
   author = {Georgios Mousmoulis and Nilla Karlsen-Davies and George Aggidis and Ioannis Anagnostopoulos and Dimitrios Papantonis},
   doi = {10.1016/j.euromechflu.2018.10.015},
   issn = {09977546},
   journal = {European Journal of Mechanics, B/Fluids},
   keywords = {Cavitation in centrifugal pumps,Diagnostics and prognostics,Flow visualization,Noise and vibration measurements},
   month = {5},
   pages = {300-311},
   publisher = {Elsevier Ltd},
   title = {Experimental analysis of cavitation in a centrifugal pump using acoustic emission, vibration measurements and flow visualization},
   volume = {75},
   year = {2019},
}
@article{Li2018,
   abstract = {Cavitation is a challenging flow abnormality that leads to undesirable effects on the energy performance of the centrifugal pump and the reliable operation of the pump system. The onset and mechanism of a phenomenon that results in unsteady cavitation must be realised to ensure a reliable operation of pumps under the cavitation state. This study focuses on cavitation instability at normal flow rate, at which point the unsteady cavitation occurs as the available net positive suction head (NPSHa) falls below 5.61 m for the researched pump. An ameliorative algorithm–united algorithm for cavitation vibration analysis is proposed on the basis of short time Fourier transform (STFT) and Wigner–Ville distribution (WVD). The STFT–WVD method is then tested using vibration data measured from the centrifugal pump. The relationship between vibration and suction performance indicates that the inception and development of cavitation can be effectively detected by the distribution and intensity of the united algorithm at the testing points. Intermediate frequency components at approximately 6 kHz fluctuate initially with the development of cavitation. A time–frequency characteristic is found to be conducive to monitoring the cavitation performance of centrifugal pumps.},
   author = {Yi Li and Guangwei Feng and Xiaojun Li and Qiaorui Si and Zuchao Zhu},
   doi = {10.1007/s12206-018-0918-x},
   issn = {1738494X},
   issue = {10},
   journal = {Journal of Mechanical Science and Technology},
   keywords = {Cavitation,Centrifugal pump,Energy performance,Time–frequency analysis,Vibration characteristic},
   month = {10},
   pages = {4711-4720},
   publisher = {Korean Society of Mechanical Engineers},
   title = {An experimental study on the cavitation vibration characteristics of a centrifugal pump at normal flow rate},
   volume = {32},
   year = {2018},
}
@article{Karagiovanidis2023,
   abstract = {The scope of this study is the evaluation of early detection methods for cavitation phenomena in centrifugal irrigation pumps by analyzing the produced vibration and sound signals from a low-cost sensor and data acquisition system and comparing several computational methods. Vibration data was acquired using the embedded accelerometer sensor of a smartphone device. Sound signals were obtained using the embedded microphone of the same commercial smartphone. The analysis was based on comparing the signals in different operating conditions with reference to the best efficiency operating point of the pump. In the case of vibrations, data was acquired for all three directional axes. The signals were processed by computational methods to extract the relative features in the frequency domain and use them to train an artificial neural network to be able to identify the different pump operating conditions while the cavitation phenomenon evolves. Three different classification algorithms were used to examine the most preferable approach for classifying data, namely the Classification Tree, the K-Nearest Neighbor, and the Support Vector Data algorithms. In addition, a convolutional neural network was utilized to examine the success rate of the classification when the datasets were formed as spectrograms instead. A detailed comparison of the classification algorithms and different axes was conducted. Comparing the results of the different methods for vibration and sound datasets, classification accuracy showed that in the case of vibration, the detection of cavitation in real conditions is possible, while it proves more challenging to identify cavitation conditions using sound data obtained with low-cost commercial sensors.},
   author = {Marios Karagiovanidis and Xanthoula Eirini Pantazi and Dimitrios Papamichail and Vassilios Fragos},
   doi = {10.3390/agriculture13081544},
   issn = {20770472},
   issue = {8},
   journal = {Agriculture (Switzerland)},
   keywords = {accelerometer,artificial neural network,cavitation,centrifugal pump,predictive maintenance,signal analysis},
   month = {8},
   publisher = {Multidisciplinary Digital Publishing Institute (MDPI)},
   title = {Early Detection of Cavitation in Centrifugal Pumps Using Low-Cost Vibration and Sound Sensors},
   volume = {13},
   year = {2023},
}
@misc{Binama2016,
   abstract = {Cavitation is one of the most challenging fluid flow abnormalities leading to detrimental effects on both the centrifugal pump flow behaviors and physical characteristics. Centrifugal pumps' most low pressure zones are the first cavitation victims, where cavitation manifests itself in form of pitting on the pump internal solid walls, accompanied by noise and vibration, all leading to the pump hydraulic performance degradation. In the present article, a general description of centrifugal pump performance and related parameters is presented. Based on the literature survey, some light were shed on fundamental cavitation features; where different aspects relating to cavitation in centrifugal pumps were briefly discussed.},
   author = {Maxime Binama and Alex Muhirwa and Emmanuel Bisengimana},
   journal = {Journal of Engineering Research and Applications www.ijera.com},
   keywords = {Cavitation,Centrifugal pump,Performance},
   pages = {52-63},
   title = {Cavitation Effects in Centrifugal Pumps-A Review},
   volume = {6},
   year = {2016},
}
@article{Cervantes2020,
   abstract = {In recent years, an enormous amount of research has been carried out on support vector machines (SVMs) and their application in several fields of science. SVMs are one of the most powerful and robust classification and regression algorithms in multiple fields of application. The SVM has been playing a significant role in pattern recognition which is an extensively popular and active research area among the researchers. Research in some fields where SVMs do not perform well has spurred development of other applications such as SVM for large data sets, SVM for multi classification and SVM for unbalanced data sets. Further, SVM has been integrated with other advanced methods such as evolve algorithms, to enhance the ability of classification and optimize parameters. SVM algorithms have gained recognition in research and applications in several scientific and engineering areas. This paper provides a brief introduction of SVMs, describes many applications and summarizes challenges and trends. Furthermore, limitations of SVMs will be identified. The future of SVMs will be discussed in conjunction with further applications. The applications of SVMs will be reviewed as well, especially in the some fields.},
   author = {Jair Cervantes and Farid Garcia-Lamont and Lisbeth Rodríguez-Mazahua and Asdrubal Lopez},
   doi = {10.1016/j.neucom.2019.10.118},
   issn = {18728286},
   journal = {Neurocomputing},
   keywords = {Classification,Machine learning,SVM},
   month = {9},
   pages = {189-215},
   publisher = {Elsevier B.V.},
   title = {A comprehensive survey on support vector machine classification: Applications, challenges and trends},
   volume = {408},
   year = {2020},
}
@article{David2021,
   abstract = {TensorFlow Lite Micro (TFLM) is an open-source ML inference framework for running deep-learning models on embedded systems. TFLM tackles the efficiency requirements imposed by embedded-system resource constraints and the fragmentation challenges that make cross-platform interoperability nearly impossible. The framework adopts a unique interpreter-based approach that provides flexibility while overcoming these unique challenges. In this paper, we explain the design decisions behind TFLM and describe its implementation. We present an evaluation of TFLM to demonstrate its low resource requirements and minimal run-time performance overheads.},
   author = {Robert David and Jared Duke and Advait Jain and Vijay Janapa Reddi and Nat Jeffries and Jian Li and Nick Kreeger and Ian Nappier and Meghna Natraj and Shlomi Regev and Rocky Rhodes and Tiezhen Wang and Pete Warden},
   journal = {Proc. Machine Learning and Systems},
   pages = {800-811},
   title = {Tensorflow lite micro: Embedded machine learning for tinyml systems},
   volume = {3},
   year = {2021},
}
@article{Lai2018,
   abstract = {Deep Neural Networks are becoming increasingly popular in always-on IoT edge devices performing data analytics right at the source, reducing latency as well as energy consumption for data communication. This paper presents CMSIS-NN, efficient kernels developed to maximize the performance and minimize the memory footprint of neural network (NN) applications on Arm Cortex-M processors targeted for intelligent IoT edge devices. Neural network inference based on CMSIS-NN kernels achieves 4.6X improvement in runtime/throughput and 4.9X improvement in energy efficiency.},
   author = {Liangzhen Lai and Naveen Suda and Vikas Chandra},
   journal = {arXiv preprint arXiv:1801.06601},
   month = {1},
   title = {CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs},
   url = {http://arxiv.org/abs/1801.06601},
   year = {2018},
}
@misc{Hasanpour2024Code,
   author = {Mohammad Amin Hasanpour},
   title = {Cavitation},
   url = {https://github.com/Black3rror/Cavitation},
   year = {2024},
}
@article{Elsken2019,
   abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
   author = {Thomas Elsken and Jan Hendrik Metzen and Frank Hutter},
   journal = {Journal of Machine Learning Research},
   keywords = {AutoDL,AutoML,Neural Architecture Search,Performance Estimation Strategy,Search Space Design,Search Strategy},
   pages = {1-21},
   title = {Neural Architecture Search: A Survey},
   volume = {20},
   year = {2019},
}
@article{Liu2018,
   abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
   author = {Hanxiao Liu and Karen Simonyan and Yiming Yang},
   journal = {arXiv preprint arXiv:1806.09055},
   month = {6},
   title = {DARTS: Differentiable Architecture Search},
   url = {http://arxiv.org/abs/1806.09055},
   year = {2018},
}
@article{Cai2018,
   abstract = {Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. $10^4$ GPU hours) makes it difficult to \emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \emph{ProxylessNAS} that can \emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6$\times$ fewer parameters. On ImageNet, our model achieves 3.1\% better top-1 accuracy than MobileNetV2, while being 1.2$\times$ faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.},
   author = {Han Cai and Ligeng Zhu and Song Han},
   journal = {arXiv preprint arXiv:1812.00332},
   month = {12},
   title = {ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware},
   year = {2018},
}
@article{Cai2019,
   abstract = {We address the challenging problem of efficient inference across many devices and resource constraints, especially on edge devices. Conventional approaches either manually design or use neural architecture search (NAS) to find a specialized neural network and train it from scratch for each case, which is computationally prohibitive (causing $CO_2$ emission as much as 5 cars' lifetime) thus unscalable. In this work, we propose to train a once-for-all (OFA) network that supports diverse architectural settings by decoupling training and search, to reduce the cost. We can quickly get a specialized sub-network by selecting from the OFA network without additional training. To efficiently train OFA networks, we also propose a novel progressive shrinking algorithm, a generalized pruning method that reduces the model size across many more dimensions than pruning (depth, width, kernel size, and resolution). It can obtain a surprisingly large number of sub-networks ($> 10^{19}$) that can fit different hardware platforms and latency constraints while maintaining the same level of accuracy as training independently. On diverse edge devices, OFA consistently outperforms state-of-the-art (SOTA) NAS methods (up to 4.0\% ImageNet top1 accuracy improvement over MobileNetV3, or same accuracy but 1.5x faster than MobileNetV3, 2.6x faster than EfficientNet w.r.t measured latency) while reducing many orders of magnitude GPU hours and $CO_2$ emission. In particular, OFA achieves a new SOTA 80.0\% ImageNet top-1 accuracy under the mobile setting ($<$600M MACs). OFA is the winning solution for the 3rd Low Power Computer Vision Challenge (LPCVC), DSP classification track and the 4th LPCVC, both classification track and detection track. Code and 50 pre-trained models (for many devices & many latency constraints) are released at https://github.com/mit-han-lab/once-for-all.},
   author = {Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},
   journal = {arXiv preprint arXiv:1908.09791},
   month = {8},
   title = {Once-for-All: Train One Network and Specialize it for Efficient Deployment},
   url = {http://arxiv.org/abs/1908.09791},
   year = {2019},
}
@article{Liu2017,
   abstract = {The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint ; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architec-tures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming , which takes wide and large networks as input models , but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20× reduction in model size and a 5× reduction in computing operations.},
   author = {Zhuang Liu and Jianguo Li and Zhiqiang Shen and Gao Huang and Shoumeng Yan and Changshui Zhang},
   journal = {IEEE international conference on computer vision},
   pages = {2736-2744},
   title = {Learning Efficient Convolutional Networks through Network Slimming},
   year = {2017},
}
@article{Hinton2015,
   abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
   author = {Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
   journal = {arXiv preprint arXiv:1503.02531},
   month = {3},
   title = {Distilling the Knowledge in a Neural Network},
   url = {http://arxiv.org/abs/1503.02531},
   year = {2015},
}
@article{Banbury2021,
   abstract = {Executing machine learning workloads locally on resource constrained microcontrollers (MCUs) promises to drastically expand the application space of IoT. However, so-called TinyML presents severe technical challenges, as deep neural network inference demands a large compute and memory budget. To address this challenge, neural architecture search (NAS) promises to help design accurate ML models that meet the tight MCU memory, latency and energy constraints. A key component of NAS algorithms is their latency/energy model, i.e., the mapping from a given neural network architecture to its inference latency/energy on an MCU. In this paper, we observe an intriguing property of NAS search spaces for MCU model design: on average, model latency varies linearly with model operation (op) count under a uniform prior over models in the search space. Exploiting this insight, we employ differentiable NAS (DNAS) to search for models with low memory usage and low op count, where op count is treated as a viable proxy to latency. Experimental results validate our methodology, yielding our MicroNet models, which we deploy on MCUs using Tensorflow Lite Micro, a standard open-source NN inference runtime widely used in the TinyML community. MicroNets demonstrate state-of-the-art results for all three TinyMLperf industry-standard benchmark tasks: visual wake words, audio keyword spotting, and anomaly detection. Models and training scripts can be found at github.com/ARM-software/ML-zoo.},
   author = {Colby Banbury and Chuteng Zhou and Igor Fedorov and Ramon Matas Navarro and Urmish Thakker and Dibakar Gope and Vijay Janapa Reddi and Matthew Mattina and Paul N. Whatmough},
   journal = {Proc machine learning and systems},
   month = {10},
   pages = {517-532},
   title = {MicroNets: Neural Network Architectures for Deploying TinyML Applications on Commodity Microcontrollers},
   volume = {3},
   url = {http://arxiv.org/abs/2010.11267},
   year = {2021},
}
@article{Ghanathe2023,
   abstract = {Deploying Machine learning (ML) on milliwatt-scale edge devices (tinyML) is gaining popularity due to recent breakthroughs in ML and Internet of Things (IoT). Most tinyML research focuses on model compression techniques that trade accuracy (and model capacity) for compact models to fit into the KB-sized tiny-edge devices. In this paper, we show how such models can be enhanced by the addition of an early exit intermediate classifier. If the intermediate classifier exhibits sufficient confidence in its prediction, the network exits early thereby, resulting in considerable savings in time. Although early exit classifiers have been proposed in previous work, these previous proposals focus on large networks, making their techniques suboptimal/impractical for tinyML applications. Our technique is optimized specifically for tiny-CNN sized models. In addition, we present a method to alleviate the effect of network overthinking by leveraging the representations learned by the early exit. We evaluate T-RecX on three CNNs from the MLPerf tiny benchmark suite for image classification, keyword spotting and visual wake word detection tasks. Our results show that T-RecX 1) improves the accuracy of baseline network, 2) achieves 31.58% average reduction in FLOPS in exchange for one percent accuracy across all evaluated models. Furthermore, we show that our methods consistently outperform popular prior works on the tiny-CNNs we evaluate.},
   author = {Nikhil P Ghanathe and Steve Wilton},
   journal = {Proc. ACM International Conference on Computing Frontiers},
   month = {7},
   pages = {123-133},
   title = {T-RECX: Tiny-Resource Efficient Convolutional neural networks with early-eXit},
   year = {2023},
}
@book{GrundfosBook,
   author = {Christian Brix Jacobsen},
   city = {Bjerringbro},
   edition = {1},
   month = {9},
   publisher = {Grundfos Management A/S},
   title = {The Centrifugal Pump},
   year = {2008},
}
@article{Njor2023,
   abstract = {Neural Architecture Search (NAS) is a popular tool for automatically generating Neural Network (NN) architectures. In early NAS works, these tools typically optimized NN architectures for a single metric, such as accuracy. However, in the case of resource constrained Machine Learning, one single metric is not enough to evaluate a NN architecture. For example, a NN model achieving a high accuracy is not useful if it does not fit inside the flash memory of a given system. Therefore, recent works on NAS for resource constrained systems have investigated various approaches to optimize for multiple metrics. In this paper, we propose that, on top of these approaches, it could be beneficial for NAS optimization of resource constrained systems to also consider input data granularity. We name such a system "Data Aware NAS", and we provide experimental evidence of its benefits by comparing it to traditional NAS.},
   author = {Emil Njor and Jan Madsen and Xenofon Fafoutis},
   journal = {TinyML Research Symposium},
   month = {4},
   title = {Data Aware Neural Architecture Search},
   url = {https://arxiv.org/abs/2304.01821},
   year = {2023},
}
@inproceedings{elsts2018board,
  title={On-board feature extraction from acceleration data for activity recognition},
  author={Elsts, Atis and McConville, Ryan and Fafoutis, Xenofon and Twomey, Niall and Piechocki, Robert and Santos-Rodriguez, Raul and Craddock, Ian},
  booktitle={EWSN’18: Proceedings of the 2018 International Conference on Embedded Wireless Systems and Networks},
  pages={163--186},
  year={2018},
  organization={ACM}
}

% ==============================================================================
% Paper EdgeMark
% ==============================================================================

@article{David2020,
   abstract = {Deep learning inference on embedded devices is a burgeoning field with myriad applications because tiny embedded devices are omnipresent. But we must overcome major challenges before we can benefit from this opportunity. Embedded processors are severely resource constrained. Their nearest mobile counterparts exhibit at least a 100 -- 1,000x difference in compute capability, memory availability, and power consumption. As a result, the machine-learning (ML) models and associated ML inference framework must not only execute efficiently but also operate in a few kilobytes of memory. Also, the embedded devices' ecosystem is heavily fragmented. To maximize efficiency, system vendors often omit many features that commonly appear in mainstream systems, including dynamic memory allocation and virtual memory, that allow for cross-platform interoperability. The hardware comes in many flavors (e.g., instruction-set architecture and FPU support, or lack thereof). We introduce TensorFlow Lite Micro (TF Micro), an open-source ML inference framework for running deep-learning models on embedded systems. TF Micro tackles the efficiency requirements imposed by embedded-system resource constraints and the fragmentation challenges that make cross-platform interoperability nearly impossible. The framework adopts a unique interpreter-based approach that provides flexibility while overcoming these challenges. This paper explains the design decisions behind TF Micro and describes its implementation details. Also, we present an evaluation to demonstrate its low resource requirement and minimal run-time performance overhead.},
   author = {Robert David and Jared Duke and Advait Jain and Vijay Janapa Reddi and Nat Jeffries and Jian Li and Nick Kreeger and Ian Nappier and Meghna Natraj and Shlomi Regev and Rocky Rhodes and Tiezhen Wang and Pete Warden},
   journal = {Proceedings of Machine Learning and Systems 3 (MLSys 2021)},
   month = {10},
   title = {TensorFlow Lite Micro: Embedded Machine Learning on TinyML Systems},
   year = {2020},
}
@inproceedings{MLonMCU,
   author = {Philipp van Kempen and Rafael Stahl and Daniel Mueller-Gritschneder and Ulf Schlichtmann},
   city = {New York, NY, USA},
   doi = {10.1145/3615338.3618128},
   isbn = {9798400703379},
   booktitle = {Proceedings of the 2023 Workshop on Compilers, Deployment, and Tooling for Edge AI},
   month = {9},
   pages = {32-36},
   publisher = {ACM},
   title = {MLonMCU: TinyML Benchmarking with Fast Retargeting},
   year = {2023},
}
@misc{EdgeImpulseEON,
   author = {Edgeimpulse},
   title = {Introducing EON: Neural Networks in Up to 55\% Less RAM and 35\% Less ROM},
   url = {https://edgeimpulse.com/blog/introducing-eon},
   note = {accessed: 16 January, 2025},
}
@misc{MicroTVM,
   author = {{Apache TVM}},
   title = {microTVM: TVM on bare-metal},
   url = {https://tvm.apache.org/docs/topic/microtvm},
   note = {accessed: 16 January, 2025},
}
@misc{Weber2020,
   author = {Logan Weber and Andrew Reusch},
   month = {6},
   title = {TinyML - How TVM is Taming Tiny},
   url = {https://tvm.apache.org/2020/06/04/tinyml-how-tvm-is-taming-tiny},
   year = {2020},
   note = {accessed: 16 January, 2025},
}
@misc{STM32CubeAI,
   author = {STMicroelectronics},
   title = {STM32Cube.AI - STMicroelectronics - STM32 AI},
   url = {https://stm32ai.st.com/stm32-cube-ai},
   note = {accessed: 16 January, 2025},
}
@misc{STM32CubeAISolution,
   author = {STMicroelectronics},
   title = {Microcontrollers STM32CubeAI Solution},
   url = {https://www.st.com/content/ccc/resource/sales_and_marketing/presentation/product_presentation/group0/69/82/bf/ae/5a/8b/40/91/STM32CubeAI_press_pres/files/STM32CubeAI_press_pres.pdf/jcr:content/translations/en.STM32CubeAI_press_pres.pdf},
   note = {accessed: 16 January, 2025},
}
@misc{eAITranslator,
   author = {{Renesas Electronics Corporation}},
   title = {e-AI Development Environment for Microcontrollers},
   url = {https://www.renesas.com/en/e-ai-development-environment-microcontrollers},
   note = {accessed: 16 January, 2025},
}
@misc{Ekkono,
   author = {Ekkono},
   title = {Edge Machine Learning and Virtual Sensors - Ekkono Solutiona},
   url = {https://www.ekkono.ai},
   note = {accessed: 16 January, 2025},
}
@misc{ArmNN,
   author = {Arm},
   title = {Arm NN SDK | Efficient ML for Arm CPUs, GPUs, \& NPUs - Arm},
   url = {https://www.arm.com/products/silicon-ip-cpu/ethos/arm-nn},
   note = {accessed: 16 January, 2025},
}
@misc{ELL,
   author = {{Microsoft Corporation}},
   title = {The Embedded Learning Library - Embedded Learning Library (ELL)},
   url = {https://microsoft.github.io/ELL},
   note = {accessed: 16 January, 2025},
}
@misc{NanoEdgeAIStudio,
   author = {STMicroelectronics},
   title = {NanoEdgeAIStudio - Automated Machine Learning (ML) tool for STM32 developers - STMicroelectronics},
   url = {https://www.st.com/en/development-tools/nanoedgeaistudio.html},
   note = {accessed: 16 January, 2025},
}
@misc{NanoEdgeAINews,
   author = {STMicroelectronics},
   title = {STMicroelectronics breaks down barriers to edge AI adoption with free NanoEdge AI deployment - ST News},
   url = {https://newsroom.st.com/media-center/press-item.html/n4592.html},
   note = {accessed: 16 January, 2025},
}
@misc{uTensor,
   author = {uTensor},
   title = {microTensor},
   url = {https://utensor.github.io/website},
   note = {accessed: 16 January, 2025},
}
@misc{Shelby2019,
   author = {Zach Shelby},
   month = {5},
   title = {uTensor and Tensor Flow Announcement | Mbed},
   url = {https://os.mbed.com/blog/entry/uTensor-and-Tensor-Flow-Announcement},
   year = {2019},
   note = {accessed: 16 January, 2025},
}
@misc{MbedUpdate,
   author = {Will Lord},
   title = {Important Update on Mbed | Mbed},
   url = {https://os.mbed.com/blog/entry/Important-Update-on-Mbed},
   note = {accessed: 16 January, 2025},
}
@misc{MicromindToolkit,
   author = {Francesco Paissan},
   title = {micromind: A toolkit for tinyML research and deployment},
   url = {https://github.com/micromind-toolkit/micromind},
   note = {accessed: 16 January, 2025},
}
@misc{ExecuTorch,
   author = {{The PyTorch Foundation}},
   title = {PyTorch ExecuTorch | PyTorch},
   url = {https://pytorch.org/executorch-overview},
   note = {accessed: 16 January, 2025},
}
@misc{Imagimob,
   author = {{Imagimob AB}},
   title = {Imagimob - Edge AI | tinyML | Deep learning},
   url = {https://www.imagimob.com},
   note = {accessed: 16 January, 2025},
}
@misc{OmniML,
   author = {Arm},
   title = {OmniML Inc. - Arm},
   url = {https://www.arm.com/partners/catalog/omnimlinc},
   note = {accessed: 16 January, 2025},
}
@inproceedings{Jaiswal2023,
   author = {Shikhar Jaiswal and Rahul Kranti Kiran Goli and Aayan Kumar and Vivek Seshadri and Rahul Sharma},
   city = {New York, NY, USA},
   doi = {10.1145/3589610.3596278},
   isbn = {9798400701740},
   booktitle = {Proceedings of the 24th ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
   month = {6},
   pages = {26-39},
   publisher = {ACM},
   title = {MinUn: Accurate ML Inference on Microcontrollers},
   year = {2023},
}
@article{Wang2019,
   abstract = {The growing number of low-power smart devices in the Internet of Things is coupled with the concept of "Edge Computing", that is moving some of the intelligence, especially machine learning, towards the edge of the network. Enabling machine learning algorithms to run on resource-constrained hardware, typically on low-power smart devices, is challenging in terms of hardware (optimized and energy-efficient integrated circuits), algorithmic and firmware implementations. This paper presents FANN-on-MCU, an open-source toolkit built upon the Fast Artificial Neural Network (FANN) library to run lightweight and energy-efficient neural networks on microcontrollers based on both the ARM Cortex-M series and the novel RISC-V-based Parallel Ultra-Low-Power (PULP) platform. The toolkit takes multi-layer perceptrons trained with FANN and generates code targeted at execution on low-power microcontrollers either with a floating-point unit (i.e., ARM Cortex-M4F and M7F) or without (i.e., ARM Cortex M0-M3 or PULP-based processors). This paper also provides an architectural performance evaluation of neural networks on the most popular ARM Cortex-M family and the parallel RISC-V processor called Mr. Wolf. The evaluation includes experimental results for three different applications using a self-sustainable wearable multi-sensor bracelet. Experimental results show a measured latency in the order of only a few microseconds and a power consumption of few milliwatts while keeping the memory requirements below the limitations of the targeted microcontrollers. In particular, the parallel implementation on the octa-core RISC-V platform reaches a speedup of 22x and a 69% reduction in energy consumption with respect to a single-core implementation on Cortex-M4 for continuous real-time classification.},
   author = {Xiaying Wang and Michele Magno and Lukas Cavigelli and Luca Benini},
   journal = {IEEE Internet of Things Journal},
   month = {11},
   title = {FANN-on-MCU: An Open-Source Toolkit for Energy-Efficient Neural Network Inference at the Edge of the Internet of Things},
   year = {2019},
}
@misc{FraunhoferAIfES,
   author = {{Fraunhofer Institute for Microelectronic Circuits and Systems}},
   title = {Artificial Intelligence for Embedded Systems - Fraunhofer IMS},
   url = {https://www.ims.fraunhofer.de/en/Business-Unit/Industry/Industrial-AI/Artificial-Intelligence-for-Embedded-Systems-AIfES.html},
   note = {accessed: 16 January, 2025},
}
@misc{Salerno2020,
   author = {Simone Salerno},
   title = {tinymlgen: Generate C code for microcontrollers from Tensorflow models},
   url = {https://github.com/eloquentarduino/tinymlgen},
   year = {2020},
   note = {accessed: 16 January, 2025},
}
@misc{Moretti2016,
   author = {Caio B. Moretti},
   title = {Neurona: Artificial Neural Networks for Arduino},
   url = {https://github.com/moretticb/Neurona},
   year = {2016},
   note = {accessed: 16 January, 2025},
}
@misc{HLS4ML,
   author = {{Fast Machine Learning Lab}},
   title = {hls4ml 1.0.0 documentation},
   url = {https://fastmachinelearning.org/hls4ml/},
   note = {accessed: 16 January, 2025},
}
@misc{MLTK,
   author = {{Silicon Labs}},
   title = {Silicon Labs Machine Learning Toolkit (MLTK) — MLTK 0.20.0 documentation},
   url = {https://siliconlabs.github.io/mltk},
   note = {accessed: 16 January, 2025},
}
@misc{CoreML,
   author = {{Apple Inc.}},
   title = {Core ML | Apple Developer Documentation},
   url = {https://developer.apple.com/documentation/coreml},
   note = {accessed: 16 January, 2025},
}
@misc{Glow,
   author = {Meta},
   title = {Glow},
   url = {https://ai.meta.com/tools/glow},
   note = {accessed: 16 January, 2025},
}
@misc{EIQ,
   author = {{NXP Semiconductors}},
   title = {eIQ ML Software Development Environment | NXP Semiconductors},
   url = {https://www.nxp.com/design/design-center/software/eiq-ml-development-environment:EIQ},
   note = {accessed: 16 January, 2025},
}
@misc{ONNXRuntime,
   author = {Microsoft},
   title = {ONNX Runtime},
   url = {https://onnxruntime.ai},
   note = {accessed: 16 January, 2025},
}
@misc{Larq,
   author = {Plumerai},
   title = {Larq},
   url = {https://larq.dev},
   note = {accessed: 16 January, 2025},
}
@misc{ONNC,
   author = {{Skymizer Taiwan Inc.}},
   title = {ONNC},
   url = {https://onnc.ai},
   note = {accessed: 16 January, 2025},
}
@misc{LatentAI,
   author = {{Latent AI}},
   title = {Latent AI - Find your best model faster},
   url = {https://latentai.com},
   note = {accessed: 16 January, 2025},
}
@misc{Plumerai,
   author = {Plumerai},
   title = {Plumerai},
   url = {https://plumerai.com/},
   note = {accessed: 16 January, 2025},
}
@misc{NNTool,
   author = {{GreenWaves Technologies}},
   title = {NNTool — GAP SDK documentation},
   url = {https://greenwaves-technologies.com/manuals_gap9/gap9_sdk_doc/html/source/tools/nntool},
   note = {accessed: 16 January, 2025},
}
@misc{Morawiec2017,
   author = {Darius Morawiec},
   title = {weka-porter: Transpile trained decision trees from Weka to C, Java or JavaScript.},
   url = {https://github.com/nok/weka-porter},
   year = {2017},
   note = {accessed: 16 January, 2025},
}
@misc{M2CGen,
   author = {Nikita Titov and Iaroslav Zeigerman and Viktor Yershov},
   title = {m2cgen: Transform ML models into a native code (Java, C, Python, Go, JavaScript, Visual Basic, C\#, R, PowerShell, PHP, Dart, Haskell, Ruby, F\#, Rust) with zero dependencies},
   url = {https://github.com/BayesWitnesses/m2cgen},
   note = {accessed: 16 January, 2025},
}
@misc{MicromlgenSklearn,
   author = {Simone Salerno},
   title = {micromlgen: Generate C code for microcontrollers from Python's sklearn classifiers},
   url = {https://github.com/eloquentarduino/micromlgen},
   note = {accessed: 16 January, 2025},
}
@article{Tsutsui2022,
   author = {Lucas Tsutsui da Silva and Vinicius M. A. Souza and Gustavo E. A. P. A. Batista},
   doi = {10.1109/JSEN.2021.3128130},
   issn = {1530-437X},
   issue = {1},
   journal = {IEEE Sensors Journal},
   month = {1},
   pages = {544-554},
   title = {An Open-Source Tool for Classification Models in Resource-Constrained Hardware},
   volume = {22},
   year = {2022},
}
@misc{QeexoAutoML,
   author = {{Qeexo AutoML}},
   title = {AutoML by TDK SensEI Help Center},
   url = {https://docs.qeexo.com/guides/userguides},
   note = {accessed: 16 January, 2025},
}
@misc{GoogleDynamicQuant,
   author = {{Google - The AI Edge Authors}},
   title = {Post-training dynamic range quantization},
   url = {https://ai.google.dev/edge/litert/models/post_training_quant},
   note = {accessed: 16 January, 2025},
}
@misc{GoogleIntegerQuant,
   author = {{Google - The AI Edge Authors}},
   title = {Post-training integer quantization},
   url = {https://ai.google.dev/edge/litert/models/post_training_quant},
   note = {accessed: 16 January, 2025},
}
@misc{GoogleInt16Quant,
   author = {{Google - The AI Edge Authors}},
   title = {Post-training integer quantization with int16 activations},
   url = {https://ai.google.dev/edge/litert/models/post_training_integer_quant_16x8},
   note = {accessed: 16 January, 2025},
}
@misc{GoogleFloat16Quant,
   author = {{Google - The AI Edge Authors}},
   title = {Post-training float16 quantization},
   url = {https://ai.google.dev/edge/litert/models/post_training_float16_quant},
   note = {accessed: 16 January, 2025},
}
@article{Rusci2019,
   abstract = {This paper presents a novel end-to-end methodology for enabling the deployment of low-error deep networks on microcontrollers. To fit the memory and computational limitations of resource-constrained edge-devices, we exploit mixed low-bitwidth compression, featuring 8, 4 or 2-bit uniform quantization, and we model the inference graph with integer-only operations. Our approach aims at determining the minimum bit precision of every activation and weight tensor given the memory constraints of a device. This is achieved through a rule-based iterative procedure, which cuts the number of bits of the most memory-demanding layers, aiming at meeting the memory constraints. After a quantization-aware retraining step, the fake-quantized graph is converted into an inference integer-only model by inserting the Integer Channel-Normalization (ICN) layers, which introduce a negligible loss as demonstrated on INT4 MobilenetV1 models. We report the latency-accuracy evaluation of mixed-precision MobilenetV1 family networks on a STM32H7 microcontroller. Our experimental results demonstrate an end-to-end deployment of an integer-only Mobilenet network with Top1 accuracy of 68% on a device with only 2MB of FLASH memory and 512kB of RAM, improving by 8% the Top1 accuracy with respect to previously published 8 bit implementations for microcontrollers.},
   author = {Manuele Rusci and Alessandro Capotondi and Luca Benini},
   journal = {Proceedings of Machine Learning and Systems},
   month = {5},
   title = {Memory-Driven Mixed Low Precision Quantization For Enabling Deep Network Inference On Microcontrollers},
   year = {2019},
}
@article{Lin2020,
   author = {Ji Lin and Wei-Ming Chen and Yujun Lin and Chuang Gan and Song Han},
   journal = {Advances in Neural Information Processing Systems},
   title = {Mcunet: Tiny deep learning on iot devices},
   volume = {33},
   year = {2020},
}
@misc{Microsoft2021,
   author = {Microsoft},
   month = {12},
   title = {Neural Network Intelligence},
   url = {https://github.com/microsoft/nni},
   year = {2021},
   note = {accessed: 16 January, 2025},
}
@inproceedings{Lv2022,
   author = {Chengfei Lv and Chaoyue Niu and Renjie Gu and Xiaotang Jiang and Zhaode Wang and Bin Liu and Ziqi Wu and Qiulin Yao and Congyu Huang and Panos Huang and Tao Huang and Hui Shu and Jinde Song and Bin Zou and Peng Lan and Guohuan Xu and Fei Wu and Shaojie Tang and Fan Wu and Guihai Chen},
   city = {Carlsbad, CA},
   isbn = {978-1-939133-28-1},
   booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
   month = {7},
   pages = {249-265},
   publisher = {USENIX Association},
   title = {Walle: An End-to-End, General-Purpose, and Large-Scale Production System for Device-Cloud Collaborative Machine Learning},
   url = {https://www.usenix.org/conference/osdi22/presentation/lv},
   year = {2022},
}
@article{Coelho2020,
   author = {Claudionor N Coelho and Aki Kuusela and Hao Zhuang and Thea Aarrestad and Vladimir Loncar and Jennifer Ngadiuba and Maurizio Pierini and Sioni Summers},
   journal = {arXiv preprint arXiv:2006.10159},
   pages = {108},
   title = {Ultra low-latency, low-area inference accelerators using heterogeneous deep quantization with QKeras and hls4ml},
   year = {2020},
}
@article{Burrello2021,
   author = {A Burrello and A Garofalo and N Bruschi and G Tagliavini and D Rossi and F Conti},
   doi = {10.1109/TC.2021.3066883},
   journal = {IEEE Transactions on Computers},
   pages = {1},
   title = {DORY: Automatic End-to-End Deployment of Real-World DNNs on Low-Cost IoT MCUs},
   year = {2021},
}
@unpublished{SklearnPorter,
   author = {Darius Morawiec},
   note = {Transpile trained scikit-learn estimators to C, Java, JavaScript and others},
   title = {sklearn-porter},
   url = {https://github.com/nok/sklearn-porter},
}
@misc{Karpathy2015,
   author = {Andrej Karpathy},
   title = {char-rnn},
   year = {2015},
   howpublished={\url{https://github.com/karpathy/char-rnn} },
   note = {accessed: 16 January, 2025},
}
@misc{EmLearn,
   author = {Jon Nordby},
   doi = {10.5281/zenodo.2589394},
   month = {3},
   title = {emlearn: Machine Learning inference engine for Microcontrollers and Embedded Devices},
   url = {https://doi.org/10.5281/zenodo.2589394},
   year = {2019},
}
@misc{NanoEdgeAIReq,
   author = {STMicroelectronics},
   title = {AI:NanoEdge AI Studio - stm32mcu},
   url = {https://wiki.st.com/stm32mcu/wiki/AI:NanoEdge_AI_Studio},
   month = {6},
   year = {2024},
   note = {accessed: 16 January, 2025},
}
@article{Njor2024,
   author = {Emil Njor and Mohammad Amin Hasanpour and Jan Madsen and Xenofon Fafoutis},
   doi = {10.1109/ACCESS.2024.3512860},
   issn = {2169-3536},
   journal = {IEEE Access},
   month = {12},
   pages = {1-1},
   title = {A Holistic Review of the TinyML Stack for Predictive Maintenance},
   year = {2024},
}
@article{Sanchez2020,
   author = {Ramon Sanchez-Iborra and Antonio F. Skarmeta},
   doi = {10.1109/MCAS.2020.3005467},
   issn = {1531-636X},
   issue = {3},
   journal = {IEEE Circuits and Systems Magazine},
   month = {8},
   pages = {4-18},
   title = {TinyML-Enabled Frugal Smart Objects: Challenges and Opportunities},
   volume = {20},
   year = {2020},
}
@article{Tsoukas2024,
   abstract = {<p>Tiny Machine Learning (TinyML) is an emerging technology proposed by the scientific community for developing autonomous and secure devices that can gather, process, and provide results without transferring data to external entities. The technology aims to democratize AI by making it available to more sectors and contribute to the digital revolution of intelligent devices. In this work, a classification of the most common optimization techniques for Neural Network compression is conducted. Additionally, a review of the development boards and TinyML software is presented. Furthermore, the work provides educational resources, a classification of the technology applications, and future directions and concludes with the challenges and considerations.</p>},
   author = {Vasileios Tsoukas and Anargyros Gkogkidis and Eleni Boumpa and Athanasios Kakarountas},
   doi = {10.1145/3661820},
   issn = {0360-0300},
   issue = {10},
   journal = {ACM Computing Surveys},
   month = {10},
   pages = {1-37},
   title = {A Review on the emerging technology of TinyML},
   volume = {56},
   year = {2024},
}
@article{Ray2022,
   author = {Partha Pratim Ray},
   doi = {10.1016/j.jksuci.2021.11.019},
   issn = {13191578},
   issue = {4},
   journal = {Journal of King Saud University - Computer and Information Sciences},
   month = {4},
   pages = {1595-1623},
   title = {A review on TinyML: State-of-the-art and prospects},
   volume = {34},
   year = {2022},
}
@article{Immonen2022,
   abstract = {<p>We use 250 billion microcontrollers daily in electronic devices that are capable of running machine learning models inside them. Unfortunately, most of these microcontrollers are highly constrained in terms of computational resources, such as memory usage or clock speed. These are exactly the same resources that play a key role in teaching and running a machine learning model with a basic computer. However, in a microcontroller environment, constrained resources make a critical difference. Therefore, a new paradigm known as tiny machine learning had to be created to meet the constrained requirements of the embedded devices. In this review, we discuss the resource optimization challenges of tiny machine learning and different methods, such as quantization, pruning, and clustering, that can be used to overcome these resource difficulties. Furthermore, we summarize the present state of tiny machine learning frameworks, libraries, development environments, and tools. The benchmarking of tiny machine learning devices is another thing to be concerned about; these same constraints of the microcontrollers and diversity of hardware and software turn to benchmark challenges that must be resolved before it is possible to measure performance differences reliably between embedded devices. We also discuss emerging techniques and approaches to boost and expand the tiny machine learning process and improve data privacy and security. In the end, we form a conclusion about tiny machine learning and its future development.</p>},
   author = {Riku Immonen and Timo Hämäläinen},
   doi = {10.1155/2022/7437023},
   issn = {1687-7268},
   journal = {Journal of Sensors},
   month = {11},
   pages = {1-11},
   title = {Tiny Machine Learning for Resource-Constrained Microcontrollers},
   volume = {2022},
   year = {2022},
}
@article{Saha2022,
   author = {Swapnil Sayan Saha and Sandeep Singh Sandha and Mani Srivastava},
   doi = {10.1109/JSEN.2022.3210773},
   issn = {1530-437X},
   issue = {22},
   journal = {IEEE Sensors Journal},
   month = {11},
   pages = {21362-21390},
   title = {Machine Learning for Microcontroller-Class Hardware: A Review},
   volume = {22},
   year = {2022},
}
@article{Capogrosso2024,
   author = {Luigi Capogrosso and Federico Cunico and Dong Seon Cheng and Franco Fummi and Marco Cristani},
   doi = {10.1109/ACCESS.2024.3365349},
   issn = {2169-3536},
   journal = {IEEE Access},
   pages = {23406-23426},
   title = {A Machine Learning-Oriented Survey on Tiny Machine Learning},
   volume = {12},
   year = {2024},
}
@article{Abadade2023,
   author = {Youssef Abadade and Anas Temouden and Hatim Bamoumen and Nabil Benamar and Yousra Chtouki and Abdelhakim Senhaji Hafid},
   doi = {10.1109/ACCESS.2023.3294111},
   issn = {2169-3536},
   journal = {IEEE Access},
   pages = {96892-96922},
   title = {A Comprehensive Survey on TinyML},
   volume = {11},
   year = {2023},
}
@article{Banbury2020,
   abstract = {Recent advancements in ultra-low-power machine learning (TinyML) hardware promises to unlock an entirely new class of smart applications. However, continued progress is limited by the lack of a widely accepted benchmark for these systems. Benchmarking allows us to measure and thereby systematically compare, evaluate, and improve the performance of systems and is therefore fundamental to a field reaching maturity. In this position paper, we present the current landscape of TinyML and discuss the challenges and direction towards developing a fair and useful hardware benchmark for TinyML workloads. Furthermore, we present our four benchmarks and discuss our selection methodology. Our viewpoints reflect the collective thoughts of the TinyMLPerf working group that is comprised of over 30 organizations.},
   author = {Colby R. Banbury and Vijay Janapa Reddi and Max Lam and William Fu and Amin Fazel and Jeremy Holleman and Xinyuan Huang and Robert Hurtado and David Kanter and Anton Lokhmotov and David Patterson and Danilo Pau and Jae-sun Seo and Jeff Sieracki and Urmish Thakker and Marian Verhelst and Poonam Yadav},
   journal = {ArXiv},
   month = {3},
   title = {Benchmarking TinyML Systems: Challenges and Direction},
   year = {2020},
}
@article{Baciu2024,
   author = {Vlad-Eusebiu Baciu and Johan Stiens and Bruno da Silva},
   doi = {10.1016/j.sysarc.2024.103262},
   issn = {13837621},
   journal = {Journal of Systems Architecture},
   month = {10},
   pages = {103262},
   title = {MLino bench: A comprehensive benchmarking tool for evaluating ML models on edge devices},
   volume = {155},
   year = {2024},
}
@article{Wulfert2024,
   author = {Lars Wulfert and Johannes Kühnel and Lukas Krupp and Justus Viga and Christian Wiede and Pierre Gembaczka and Anton Grabmaier},
   doi = {10.1109/TPAMI.2024.3355495},
   issn = {0162-8828},
   issue = {6},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   month = {6},
   pages = {4519-4533},
   title = {AIfES: A Next-Generation Edge AI Framework},
   volume = {46},
   year = {2024},
}
@InProceedings{Osman2022,
author="Osman, Anas
and Abid, Usman
and Gemma, Luca
and Perotto, Matteo
and Brunelli, Davide",
editor="Saponara, Sergio
and De Gloria, Alessandro",
title="TinyML Platforms Benchmarking",
booktitle="Applications in Electronics Pervading Industry, Environment and Society",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="139--148",
abstract="Recent advances in state-of-the-art ultra-low power embedded devices for machine learning (ML) have permitted a new class of products whose key features enable ML capabilities on microcontrollers with less than 1 mW power consumption (TinyML). TinyML provides a unique solution by aggregating and analyzing data at the edge on low-power embedded devices. However, we have only recently been able to run ML on microcontrollers, and the field is still in its infancy, which means that hardware, software, and research are changing extremely rapidly. Consequently, many TinyML frameworks have been developed for different platforms to facilitate the deployment of ML models and standardize the process. Therefore, in this paper, we focus on benchmarking two popular frameworks: Tensorflow Lite Micro (TFLM) on the Arduino Nano BLE and CUBE AI on the STM32-NucleoF401RE to provide a standardized framework selection criterion for specific applications.",
isbn="978-3-030-95498-7"
}
@inproceedings{liu2021deep,
  title={Deep Architecture Compression with Automatic Clustering of Similar Neurons},
  author={Liu, Xiang and Liu, Wenxue and Wang, Li-Na and Zhong, Guoqiang},
  booktitle={Pattern Recognition and Computer Vision: 4th Chinese Conference, PRCV 2021, Beijing, China, October 29--November 1, 2021, Proceedings, Part IV 4},
  pages={361--373},
  year={2021},
  organization={Springer}
}
@article{hinton2015distilling,
  title   = {Distilling the knowledge in a neural network},
  author  = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal = {arXiv preprint arXiv:1503.02531},
  volume  = {2},
  number  = {7},
  year    = {2015}
}
@inproceedings{ghanathe2022t,
  title={T-recx: Tiny-resource efficient convolutional neural networks with early-exit},
  author={Ghanathe, Nikhil P and Wilton, Steve},
  booktitle={Proceedings of the 20th ACM International Conference on Computing Frontiers},
  pages={123--133},
  year={2023}
}
@inproceedings{song2022,
 author = {Lin, Ji and Zhu, Ligeng and Chen, Wei-Ming and Wang, Wei-Chen and Gan, Chuang and Han, Song},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {22941--22954},
 publisher = {Curran Associates, Inc.},
 title = {On-Device Training Under 256KB Memory},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/90c56c77c6df45fc8e556a096b7a2b2e-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}
@misc{hymel2023edgeimpulsemlopsplatform,
   title={Edge Impulse: An MLOps Platform for Tiny Machine Learning}, 
   author={Shawn Hymel and Colby Banbury and Daniel Situnayake and Alex Elium and Carl Ward and Mat Kelcey and Mathijs Baaijens and Mateusz Majchrzycki and Jenny Plunkett and David Tischler and Alessandro Grande and Louis Moreau and Dmitry Maslov and Artie Beavis and Jan Jongboom and Vijay Janapa Reddi},
   year={2023},
   eprint={2212.03332},
   archivePrefix={arXiv},
   primaryClass={cs.DC},
   url={https://arxiv.org/abs/2212.03332}, 
}
@INPROCEEDINGS{Hasanpour2024,
  author={Hasanpour, Mohammad Amin and Engholm, Rasmus and Fafoutis, Xenofon},
  booktitle={2024 IEEE Annual Congress on Artificial Intelligence of Things (AIoT)}, 
  title={Pump Cavitation Detection with Machine Learning: A Comparative Study of SVM and Deep Learning}, 
  year={2024},
  volume={},
  number={},
  pages={219-225},
  keywords={Support vector machines;Deep learning;Adaptation models;Analytical models;Pumps;Predictive models;Hardware;cavitation;support vector machine;deep learning;machine learning;centrifugal pump},
  doi={10.1109/AIoT63253.2024.00050}
}
@article{pavan2024tybox,
  title={TyBox: An Automatic Design and Code Generation Toolbox for TinyML Incremental On-Device Learning},
  author={Pavan, Massimo and Ostrovan, Eugeniu and Caltabiano, Armando and Roveri, Manuel},
  journal={ACM Transactions on Embedded Computing Systems},
  volume={23},
  number={3},
  pages={1--27},
  year={2024},
  publisher={ACM New York, NY}
}
@inproceedings{ren2021tinyol,
  title={Tinyol: Tinyml with online-learning on microcontrollers},
  author={Ren, Haoyu and Anicic, Darko and Runkler, Thomas A},
  booktitle={2021 international joint conference on neural networks (IJCNN)},
  pages={1--8},
  year={2021},
  organization={IEEE}
}
@inproceedings{howard2019searching,
  title={Searching for mobilenetv3},
  author={Howard, Andrew and Sandler, Mark and Chu, Grace and Chen, Liang-Chieh and Chen, Bo and Tan, Mingxing and Wang, Weijun and Zhu, Yukun and Pang, Ruoming and Vasudevan, Vijay and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1314--1324},
  year={2019}
}
@article{cai2019once,
  title={Once-for-all: Train one network and specialize it for efficient deployment},
  author={Cai, Han and Gan, Chuang and Wang, Tianzhe and Zhang, Zhekai and Han, Song},
  journal={arXiv preprint arXiv:1908.09791},
  year={2019}
}
@inproceedings{gambella2022cnas,
  title={CNAS: Constrained Neural Architecture Search},
  author={Gambella, Matteo and Falcetta, Alessandro and Roveri, Manuel},
  booktitle={2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC)},
  pages={2918--2923},
  year={2022},
  organization={IEEE}
}
@article{lomurno2024pomonag,
  title={POMONAG: Pareto-Optimal Many-Objective Neural Architecture Generator},
  author={Lomurno, Eugenio and Mariani, Samuele and Monti, Matteo and Matteucci, Matteo},
  journal={arXiv preprint arXiv:2409.20447},
  year={2024}
}
@misc{TFLMSupportedOps,
   author = {{Google - The AI Edge Authors}},
   title = {Build and convert models},
   url = {https://ai.google.dev/edge/litert/microcontrollers/build_convert#operation_support},
   note = {accessed: 3 April, 2025},
}
@misc{EISupportedOps,
   author = {Edgeimpulse},
   title = {Layers},
   url = {https://docs.edgeimpulse.com/docs/concepts/machine-learning/neural-networks/layers},
   note = {accessed: 3 April, 2025},
}
@inproceedings{chen2018tvm,
  title={$\{$TVM$\}$: An automated $\{$End-to-End$\}$ optimizing compiler for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Zheng, Lianmin and Yan, Eddie and Shen, Haichen and Cowan, Meghan and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and others},
  booktitle={13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages={578--594},
  year={2018}
}

% ==============================================================================
% Paper Quantization Survey
% ==============================================================================

@article{abadade2023comprehensive,
  title={A comprehensive survey on tinyml},
  author={Abadade, Youssef and Temouden, Anas and Bamoumen, Hatim and Benamar, Nabil and Chtouki, Yousra and Hafid, Abdelhakim Senhaji},
  journal={IEEE Access},
  volume={11},
  pages={96892--96922},
  year={2023},
  publisher={IEEE}
}
@article{capogrosso2024machine,
  title={A machine learning-oriented survey on tiny machine learning},
  author={Capogrosso, Luigi and Cunico, Federico and Cheng, Dong Seon and Fummi, Franco and Cristani, Marco},
  journal={IEEE Access},
  volume={12},
  pages={23406--23426},
  year={2024},
  publisher={IEEE}
}
@article{rajapakse2023intelligence,
  title={Intelligence at the extreme edge: A survey on reformable TinyML},
  author={Rajapakse, Visal and Karunanayake, Ishan and Ahmed, Nadeem},
  journal={ACM Computing Surveys},
  volume={55},
  number={13s},
  pages={1--30},
  year={2023},
  publisher={ACM New York, NY}
}
@article{heydari2025tiny,
  title={Tiny machine learning and on-device inference: A survey of applications, challenges, and future directions},
  author={Heydari, Soroush and Mahmoud, Qusay H},
  journal={Sensors},
  volume={25},
  number={10},
  pages={3191},
  year={2025},
  publisher={MDPI}
}
@article{guo2018survey,
  title={A survey on methods and theories of quantized neural networks},
  author={Guo, Yunhui},
  journal={arXiv preprint arXiv:1808.04752},
  year={2018}
}
@article{zhuo2022empirical,
  title={An empirical study of low precision quantization for TinyML},
  author={Zhuo, Shaojie and Chen, Hongyu and Ramakrishnan, Ramchalam Kinattinkara and Chen, Tommy and Feng, Chen and Lin, Yicheng and Zhang, Parker and Shen, Liang},
  journal={arXiv preprint arXiv:2203.05492},
  year={2022}
}
@article{novac2021quantization,
  title={Quantization and deployment of deep neural networks on microcontrollers},
  author={Novac, Pierre-Emmanuel and Boukli Hacene, Ghouthi and Pegatoquet, Alain and Miramond, Benoit and Gripon, Vincent},
  journal={Sensors},
  volume={21},
  number={9},
  pages={2984},
  year={2021},
  publisher={MDPI}
}
@article{nagel2106white,
  title={A white paper on neural network quantization. arXiv 2021},
  author={Nagel, Markus and Fournarakis, Marios and Amjad, Rana Ali and Bondarenko, Yelysei and van Baalen, Mart and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2106.08295},
  volume={4},
  year={2021}
}
@misc{ibmqat,
   author = {Clark, Bryan},
   title = {What is quantization aware training (qat)?},
   url = {https://www.ibm.com/think/topics/quantization-aware-training},
   note = {accessed: 10 July, 2025},
}
@misc{quarkmixed,
   author = {{Advanced Micro Devices}},
   title = {Mixed Precision},
   url = {https://quark.docs.amd.com/latest/onnx/tutorial_mix_precision.html},
   note = {accessed: 10 July, 2025},
}
@article{njor2024holistic,
   author = {Emil Njor and Mohammad Amin Hasanpour and Jan Madsen and Xenofon Fafoutis},
   doi = {10.1109/ACCESS.2024.3512860},
   issn = {2169-3536},
   journal = {IEEE Access},
   month = {12},
   pages = {1-1},
   title = {A Holistic Review of the TinyML Stack for Predictive Maintenance},
   year = {2024},
}
@article{hasanpour2025edgemark,
title = {EdgeMark: An automation and benchmarking system for embedded artificial intelligence tools},
journal = {Journal of Systems Architecture},
volume = {167},
pages = {103488},
year = {2025},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2025.103488},
url = {https://www.sciencedirect.com/science/article/pii/S1383762125001602},
author = {Mohammad Amin Hasanpour and Mikkel Kirkegaard and Xenofon Fafoutis},
keywords = {Machine learning, TinyML, Embedded AI, Automation, Benchmarking, Microcontrollers},
abstract = {The integration of artificial intelligence (AI) into embedded devices, a paradigm known as embedded artificial intelligence (eAI) or tiny machine learning (TinyML), is transforming industries by enabling intelligent data processing at the edge. However, the many tools available in this domain leave researchers and developers wondering which one is best suited to their needs. This paper provides a review of existing eAI tools, highlighting their features, trade-offs, and limitations. Additionally, we introduce EdgeMark, an open-source automation system designed to streamline the workflow for deploying and benchmarking machine learning (ML) models on embedded platforms. EdgeMark simplifies model generation, optimization, conversion, and deployment while promoting modularity, reproducibility, and scalability. Experimental benchmarking results showcase the performance of widely used eAI tools, including TensorFlow Lite Micro (TFLM), Edge Impulse, Ekkono, and Renesas eAI Translator, across a wide range of models, revealing insights into their relative strengths and weaknesses. The findings provide guidance for researchers and developers in selecting the most suitable tools for specific application requirements, while EdgeMark lowers the barriers to adoption of eAI technologies.}
}
@misc{GitHubTFLite,
   author = {{TensorFlow Contributors}},
   title = {GitHub - TensorFlow Lite},
   url = {https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite},
   note = {accessed: 24 July, 2025},
}
@misc{GitHubPyTorchQNNPACK,
   author = {{PyTorch Contributors}},
   title = {GitHub - pytorch/QNNPACK},
   url = {https://github.com/pytorch/QNNPACK},
   note = {accessed: 24 July, 2025},
}
@misc{GitHubPyTorchQuant,
   author = {{PyTorch contributors}},
   title = {GitHub - pytorch quantization},
   url = {https://github.com/pytorch/pytorch/tree/v2.7.1/torch/ao/quantization},
   note = {accessed: 24 July, 2025},
}
@misc{GitHubQKeras,
   author = {{Google Contributors}},
   title = {GitHub - google/QKeras},
   url = {https://github.com/google/qkeras},
   note = {accessed: 24 July, 2025},
}
@misc{GitHubLarq,
   author = {{Larq Contributors}},
   title = {GitHub - larq/larq},
   url = {https://github.com/larq/larq},
   note = {accessed: 24 July, 2025},
}
@misc{GitHubBrevitas,
   author = {{Xilinx Contributors}},
   title = {GitHub - xilinx/brevitas},
   url = {https://github.com/Xilinx/brevitas},
   note = {accessed: 24 July, 2025},
}
@software{Microsoft_Neural_Network_Intelligence_2021,
author = {{Microsoft}},
month = jan,
title = {{Neural Network Intelligence}},
url = {https://github.com/microsoft/nni},
version = {2.0},
year = {2021}
}
@software{ONNX_Runtime_developers_ONNX_Runtime_2018,
author = {{ONNX Runtime developers}},
license = {MIT},
month = nov,
title = {{ONNX Runtime}},
url = {https://github.com/microsoft/onnxruntime},
year = {2018}
}
@misc{GitHubAIMET,
   author = {{Qualcomm Innovation Center}},
   title = {GitHub - quic/aimet},
   url = {https://github.com/quic/aimet},
   note = {accessed: 24 July, 2025},
}
@misc{GitHubNNCF,
   author = {{NNCF Contributors}},
   title = {GitHub - openvinotoolkit/nncf},
   url = {https://github.com/openvinotoolkit/nncf},
   note = {accessed: 24 July, 2025},
}
@misc{LiteRTQuantSpec,
   author = {{Google}},
   title = {LiteRT 8-bit quantization specification},
   url = {https://ai.google.dev/edge/litert/models/quantization_spec},
   note = {accessed: 24 July, 2025},
}
@misc{LiteRTOpt,
   author = {{Google }},
   title = {LiteRT model optimization},
   url = {https://ai.google.dev/edge/litert/models/model_optimization},
   note = {accessed: 24 July, 2025},
}
@misc{TFLiteAccelerators,
   author = {Marat Dukhan},
   title = {Accelerating TensorFlow Lite with XNNPACK Integration},
   url = {https://blog.tensorflow.org/2020/07/accelerating-tensorflow-lite-xnnpack-integration.html},
   note = {accessed: 24 July, 2025},
}
@misc{QNNPACK,
   author = {Marat Dukhan and Yiming Wu and Hao Lu},
   title = {QNNPACK: Open source library for optimized mobile deep learning},
   url = {https://engineering.fb.com/2018/10/29/ml-applications/qnnpack/},
   note = {accessed: 24 July, 2025},
}
@misc{PyTorchQuantization,
   author = {{PyTorch Contributors}},
   title = {Quantization},
   url = {https://docs.pytorch.org/docs/stable/quantization.html},
   note = {accessed: 24 July, 2025},
}
@misc{LarqDocs,
   author = {{Plumerai Developers}},
   title = {Larq - Getting started},
   url = {https://docs.larq.dev/larq/},
   note = {accessed: 24 July, 2025},
}
@article{bannink2021larq,
  title={Larq compute engine: Design, benchmark and deploy state-of-the-art binarized neural networks},
  author={Bannink, Tom and Hillier, Adam and Geiger, Lukas and de Bruin, Tim and Overweel, Leon and Neeven, Jelmer and Helwegen, Koen},
  journal={Proceedings of Machine Learning and Systems},
  volume={3},
  pages={680--695},
  year={2021}
}
@misc{PlumeraiEETimesPost,
   author = {Sally Ward-Foxton},
   title = {Startup Outperforms AI Inference Engines for Cortex-M},
   url = {https://www.eetimes.eu/dutch-startup-outperforms-ai-inference-engines-for-cortex-m/},
   year = {2021},
   note = {accessed: 24 July, 2025},
}
@misc{PlumeraiMLPerf,
   author = {{Plumerai Contributors}},
   title = {Plumerai wins MLPerf Tiny 1.1 AI benchmark for microcontrollers again},
   url = {https://blog.plumerai.com/2023/06/mlperf-tiny-1.1/},
   year = {2023},
   note = {accessed: 24 July, 2025},
}
@misc{PlumeraiDocs,
   author = {{Plumerai contributors}},
   title = {Plumerai Documentation Overview},
   url = {https://docs.plumerai.com/2.2/},
   note = {accessed: 24 July, 2025},
}
@misc{BrevitasDocs,
   author = {{Advanced Micro Devices}},
   title = {Brevitas Documentation},
   url = {https://xilinx.github.io/brevitas/v0.12.0/},
   note = {accessed: 24 July, 2025},
}
@misc{NNIDocs,
   author = {{Microsoft}},
   title = {Overview of NNI Model Quantization},
   url = {https://nni.readthedocs.io/en/stable/compression/quantization.html},
   note = {accessed: 24 July, 2025},
}
@misc{QNNMPDocs,
   author = {{Qualcomm}},
   title = {Quantization},
   url = {https://docs.qualcomm.com/bundle/publicresource/topics/80-63442-50/quantization.html},
   note = {accessed: 24 July, 2025},
}
@misc{ONNXRuntimeDocs,
   author = {{ONNX Contributors}},
   title = {Quantize ONNX models},
   url = {https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html},
   note = {accessed: 24 July, 2025},
}
@misc{AIMETDocs,
   author = {{Qualcomm Innovation Center}},
   title = {AIMET Documentation},
   url = {https://quic.github.io/aimet-pages/releases/latest/index.html},
   note = {accessed: 24 July, 2025},
}
