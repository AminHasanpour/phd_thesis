\chapter{Introduction}\label{ch:introduction}

In the last decade, \gls{ai} placed itself at the forefront of scientific research. Some call it the new \enquote{electricity}, while others describe its future as \enquote{doomed}. However, one thing is certain: regardless of differing opinions, \gls{ai} is here to stay and will continue to evolve at a rapid pace. From self-driving cars to advanced language models, \gls{ai} is transforming industries and reshaping our daily lives.

The influence of \gls{ai} is already visible across a wide range of domains, including healthcare, automation, finance, and entertainment. In the life sciences, the success of AlphaFold \cite{jumper2021highly} demonstrated how data-driven models can address long-standing challenges in protein structure prediction, significantly accelerating biological research and drug discovery. In natural language processing, recent advances in \glspl{llm} have shown an unprecedented ability to generate coherent and context-aware text, enabling new forms of interaction in areas such as content creation, education, and customer support. Financial systems increasingly rely on \gls{ai} for tasks such as fraud detection and algorithmic trading, while in entertainment, recommendation systems driven by learned user preferences have become a central component of digital platforms. Together, these examples highlight how \gls{ai} has transitioned from a primarily academic pursuit to a key enabling technology across industries.

At a conceptual level, \gls{ai} refers to the simulation of aspects of human intelligence in machines, allowing them to perform tasks that typically require cognitive capabilities such as perception, reasoning, and decision-making. Within this broad field, \gls{ml} focuses on algorithms that improve their performance by learning from data rather than relying on explicitly programmed rules. \Gls{dl}, a subset of \gls{ml}, is based on artificial neural networks composed of multiple layers that learn hierarchical representations of data. This layered structure enables deep models to capture complex, non-linear relationships, making them particularly effective for tasks such as image classification, speech recognition, and natural language understanding. In practice, the term \gls{ai} is often used as an umbrella concept encompassing both \gls{ml} and \gls{dl}, especially when referring to applied systems. In this thesis, the focus is placed primarily on \gls{ml}, and in particular on \gls{dl} techniques, with an emphasis on their deployment in embedded systems.

The theoretical foundations of \gls{dl} can be traced back several decades to early neural network models inspired by biological information processing. While these ideas generated significant interest, practical progress was limited for many years due to a lack of large-scale datasets, insufficient computational resources, and challenges associated with training deep architectures. These limitations contributed to periods of reduced research activity, commonly referred to as \enquote{AI winter}, during which alternative machine learning approaches dominated the field.

This situation changed in 2012 with the introduction of AlexNet, which demonstrated the applicability of \gls{dl} to challenging real-world problems \cite{krizhevsky2012imagenet}. Since then, \gls{dl} has experienced rapid growth and widespread adoption across various domains, particularly in academia. Numerous breakthroughs have been achieved, and industry adoption has resulted in new products and services that were previously not possible. However, industry has only begun to harvest the full potential of \gls{dl}, and it is expected that these applications will continue to grow and evolve in the coming years. This development highlights the significance of \gls{tinyml}, which serves as an enabling technology for deploying \gls{dl} models on resource-constrained devices.

\section{TinyML: Machine Learning on Embedded Devices}

There are several terms in use within the community---most notably \gls{tinyml}, \gls{eai}, and Edge AI---which are often used interchangeably to refer to closely related concepts. Although their precise definitions differ slightly, they all convey the general idea of deploying \gls{ml} models efficiently on resource-constrained devices. Within this broader interpretation, many techniques developed in this field, such as quantization or knowledge distillation, are also applicable to large-scale systems that face constraints related to power consumption, latency, or operational cost, for example \glspl{llm} deployed on servers. In this thesis, however, the focus is explicitly placed on the efficient deployment of \gls{ml} models on small-scale embedded devices with severe resource constraints, such as microcontrollers.

The main challenges addressed by \gls{tinyml} include the following:
\begin{itemize}
    \item \textbf{Energy Consumption:} Transmitting data over wireless networks consumes significantly more energy than performing local computation, often by orders of magnitude. By processing data directly on embedded devices, \gls{tinyml} reduces the need for frequent data transmission, thereby conserving energy, an essential requirement for battery-powered systems. Moreover, reduced energy consumption typically leads to lower heat generation, which can be critical in certain application scenarios.
    \item \textbf{Security and Privacy:} Transmitting sensitive data to the cloud raises concerns about data security and user privacy. \Gls{tinyml} enables local data processing, minimizing the exposure of sensitive information and reducing the risk of data breaches.
    \item \textbf{Latency:} Real-time applications, such as voice assistants and autonomous vehicles, require immediate responses. Relying on cloud-based processing can introduce latency due to network delays. \Gls{tinyml} allows for on-device inference, ensuring faster response times and improved user experience.
    \item \textbf{Connectivity:} Many embedded devices operate in environments with limited or unreliable internet connectivity. \Gls{tinyml} allows such systems to operate autonomously without continuous network access, ensuring robustness and uninterrupted functionality in remote or disconnected settings.
\end{itemize}

To achieve these goals, \gls{tinyml} leverages a range of model optimization and system-level techniques, including the following \cite{Njor2024}:
\begin{itemize}
    \item \textbf{Quantization:} Quantization reduces the numerical precision used to represent a model's weights, activations, and sometimes biases, typically converting 32-bit floating-point values into lower-bit fixed-point integers (e.g., 8-bit). This can significantly lower memory usage and computational cost, and allows models to run efficiently on microcontrollers without floating-point hardware. While quantization might slightly reduce accuracy, techniques such as quantization-aware training or mixed-precision quantization help preserve performance.
    \item \textbf{Pruning:} Pruning removes parameters (weights, neurons, filters, or channels) that contribute little to a model's output, thereby reducing model size and computation. Unstructured pruning removes individual weights and can create sparse models, while structured pruning removes entire components (e.g., filters or neurons), which is more compatible with embedded hardware. The goal is to eliminate redundancy while maintaining acceptable accuracy.
    \item \textbf{Clustering:} Clustering groups similar weights together and forces all weights in a group to share a single value. This reduces the number of unique parameters that must be stored, enabling model compression. Although clustering does not directly speed up inference, it can improve memory efficiency and cache behavior, especially when combined with pruning and quantization.
    \item \textbf{Neuron Merging:} Neuron merging exploits the fact that many neurons within the same layer learn similar features. By identifying and combining such neurons, redundancy is reduced, leading to fewer parameters and lower computational cost. Unlike pruning or clustering, neuron merging directly produces a smaller, denser network without requiring special hardware or sparse-matrix support.
    \item \textbf{Knowledge Distillation:} Knowledge distillation transfers knowledge from a large, accurate \enquote{teacher} model to a smaller, more efficient \enquote{student} model. The student is trained not only on ground-truth labels but also to mimic the teacher's output behavior. This allows compact models suitable for TinyML devices to achieve higher accuracy than if they were trained alone.
    \item \textbf{Cascading Architectures:} Cascading architectures use multiple models arranged in stages, where simpler and cheaper models run first and only invoke more complex models when necessary. This reduces average energy consumption and latency because expensive inference is performed only on difficult or uncertain inputs. Such architectures are particularly useful when most inputs are easy to classify.
    \item \textbf{Early Exit Networks:} Early exit networks add intermediate output layers to a neural network, allowing inference to terminate early if sufficient confidence is achieved. For easy inputs, the model exits sooner, saving computation and energy. More complex inputs propagate deeper into the network, trading additional cost for higher accuracy.
    \item \textbf{AutoML:} AutoML automates the search for efficient model architectures and hyperparameters under given constraints such as memory, latency, and energy. In TinyML, AutoML techniques (often including \gls{nas}) are used to discover models that are specifically optimized for resource-constrained hardware, rather than adapting large, generic architectures.
    \item \textbf{Compression:} Another approach to reducing a model's memory footprint is to apply lossless compression techniques, such as Huffman encoding. Although this can significantly decrease the amount of memory required to store the model, it also introduces additional overhead, as the model must be decompressed before execution, resulting in slower runtime performance. Compression could be combined with other techniques like pruning to maximize memory savings.
\end{itemize}

Unlike \glspl{pc} and servers, which have largely converged around a small number of standardized hardware architectures and operating systems, the microcontroller and TinyML landscape remains highly heterogeneous. Tens of millions of microcontrollers are produced by dozens of vendors, each often relying on its own CPU architecture, peripheral set, and minimal software stack. For example, in 2007 there were approximately 40 suppliers shipping around 50 incompatible MCU architectures, with no single architecture holding even 5\% of the market \cite{MCUFragmentation}. Although architectures such as ARM Cortex-M and, more recently, RISC-V have contributed to a degree of unification, the ecosystem remains far from the level of standardization observed in larger computing systems such as \glspl{pc}.

This fragmentation is intrinsic to the nature of embedded systems. Embedded devices are designed for a vast range of highly specialized applications, each with distinct requirements regarding performance, power consumption, physical size, and cost. Moreover, these systems are often subject to extremely tight cost constraints, sometimes down to individual cents, which necessitates architectures optimized for specific use cases. Such constraints leave little room for abstraction layers such as full-featured operating systems. As a result, manufacturers frequently develop custom hardware architectures and software stacks tailored to particular application domains, further contributing to the diversity of microcontroller platforms.

A similar situation exists within the TinyML ecosystem. Dozens of specialized toolkits and frameworks are available, each targeting specific combinations of models, hardware platforms, and deployment scenarios. Some toolchains provide end-to-end support, including data collection, preprocessing, inference, and postprocessing, while others focus exclusively on efficient model execution. This diversity reflects the wide range of constraints and application requirements found in embedded systems. As an illustration, Section~\ref{sec:paper-b-eAI_tools} lists approximately 50 TinyML toolkits and frameworks that are currently available.

This fragmentation brings both advantages and challenges. On the positive side, it encourages innovation and enables highly optimized, application-specific solutions. It also lowers barriers to entry, allowing new players to contribute novel ideas, and reduces monoculture risks by preventing the ecosystem from being at the mercy of one company's strategy. On the negative side, fragmentation significantly increases development complexity. Porting code or machine learning models between different microcontrollers can be difficult or even infeasible. Development teams often need to work with multiple integrated development environments, build systems, and debugging tools, and it becomes harder to reuse common libraries or operating system services.

In the context of TinyML, fragmentation also leads to confusion, as many tools provide overlapping functionality while differing in supported features, performance characteristics, and limitations. This makes selecting an appropriate toolchain for a given application a non-trivial task. This challenge directly motivated the collaboration underlying this thesis with the industrial partners Grundfos and VELUX, both of which were interested in deploying \gls{ml} models on embedded devices while seeking to make informed, future-proof decisions in the rapidly evolving field of embedded \gls{ai}.

Throughout the course of this PhD study, similar challenges were observed across several other companies and startups, which likewise struggled to identify suitable TinyML tools and deployment approaches for their specific applications. These recurring difficulties highlight a broader need for systematic guidance and comparative insight into the TinyML ecosystem.

TinyML resides at the intersection of two opposing forces: machine learning, which typically benefits from high levels of abstraction, and embedded systems, which penalize abstraction due to strict resource constraints. The result is a fragmented ecosystem of tools and techniques. However, an important unifying factor exists: the input to TinyML deployment pipelines---trained models---is typically represented using a small number of standardized formats. This creates an opportunity for orchestration systems that can take a trained model and, by leveraging hardware-specific toolchains, deploy it across a wide range of embedded platforms. In this way, the challenges posed by fragmentation can be mitigated to a significant extent.

This thesis helps scientists and engineers to understand their available options in this fragmented ecosystem and make informed decisions when selecting tools and techniques for deploying \gls{ml} models on embedded devices. Further, we introduce EdgeMark, a system designed to streamline the process of deploying models across diverse embedded platforms by bridging standardized model representations and hardware-specific toolchains. Rather than addressing fragmentation by enforcing uniformity, EdgeMark embraces heterogeneity and provides a structured way to work within it. By providing a comprehensive overview of the TinyML landscape and introducing a unified deployment framework, this work aims to facilitate the adoption and advancement of \gls{ml} on embedded systems.

\section{Thesis Outline}

This thesis is structured around three peer-reviewed papers, followed by a concluding chapter that summarizes their contributions.
\begin{itemize}
    \item \textbf{Chapter \ref{ch:paper-a-cavitation}} presents Paper~A \cite{Hasanpour2024}, which develops efficient models for cavitation detection in industrial pumps. Beyond the specific application, this work serves as a representative example demonstrating that, in many industrial scenarios, complex \gls{dl} models are not always the most appropriate solution. Instead, simpler machine learning models can be sufficient when combined with effective feature engineering. The results confirm that traditional machine learning algorithms should not be disregarded and remain a viable option for \gls{tinyml} applications, thereby expanding the overall design space of potential models. Furthermore, the paper highlights the importance of benchmarking models on target hardware, as the theoretically most efficient model may not be the most efficient in practice due to hardware and software behavior.
    \item \textbf{Chapter \ref{ch:paper-b-edgemark}} presents Paper~B \cite{hasanpour2025edgemark}, which provides a comprehensive overview of existing \gls{tinyml} tools and frameworks, highlighting their capabilities and limitations. It also introduces EdgeMark, a system designed to facilitate the generation and deployment of \gls{ml} models across multiple embedded platforms. EdgeMark currently supports a wide range of models and datasets, four different conversion tools, and two hardware platforms. Its modular design enables easy extension to additional models, datasets, tools, and hardware targets. In addition, the paper benchmarks the supported tools across a wide range of models and deployment scenarios, offering insights into their relative performance and practical limitations.
    \item \textbf{Chapter \ref{ch:paper-c-quantization}} presents Paper~C \cite{hasanpour2025survey}, which focuses on quantization in \gls{tinyml} tools. It surveys twelve widely used \gls{tinyml} frameworks with particular emphasis on their quantization dimensions, including supported quantization types, hardware compatibility, and deployment workflows. The findings are summarized in a comprehensive comparison table that enables scientists and engineers to quickly identify suitable tools for their specific application requirements. The paper further discusses current trends, trade-offs in quantization strategies, and potential future directions for quantization in embedded and edge \gls{ai} tools.
\end{itemize}

\section{Additional Publications and Research Outputs}
In addition to the papers included in this thesis, several other research outputs were produced during the course of this PhD study. While these works are related to the overall research agenda, they are not directly included in the thesis. They are listed below for completeness.
\begin{description}
    \item[Peer-reviewed article]\mbox{}\\
    \AtNextCite{\defcounter{maxnames}{99}}
    \fullcite{Njor2024}

    This work presents a comprehensive review of the TinyML stack, with particular emphasis on its application in predictive maintenance. The paper introduces a novel taxonomy for the systematic study of TinyML systems and provides an in-depth discussion of the different layers of the TinyML stack. Specifically, it covers the motivation for TinyML, its advantages and disadvantages, hardware limitations and corresponding solutions, available toolchains for both traditional \gls{ml} and \gls{dl}, model types and optimization techniques, relevant datasets, and common application areas, with a special focus on predictive maintenance.

    \item[Manuscript in preparation]\mbox{}\\
    \AtNextCite{\defcounter{maxnames}{99}}
    \fullcite{Marstrand2026}

    This work introduces a novel \gls{nas} method, termed predictor-guided \gls{dfm}, which is particularly well suited for TinyML applications. The proposed approach is able to identify architectures that are comparable to, or even outperform, their counterparts, while offering advantageous properties such as high search efficiency and continuous improvement over time. The method has also been extended to multi-objective optimization, enabling simultaneous optimization of competing objectives, for example model accuracy and memory footprint.

    \item[Publication feature]\mbox{}\\
    \fullcite{DIRECpost}

    This DIREC news feature highlights EdgeMark as a key enabling tool supporting industrial partners such as VELUX and Grundfos. It illustrates the practical relevance of the presented research and underscores the importance of such tools for the industrial adoption of TinyML technologies.

    \item[Open-source software]\mbox{}\\
    \fullcite{EdgeMarkRepo}

    \item[Open-source software]\mbox{}\\
    \fullcite{CavitationRepo}
\end{description}
